{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "from statistics import mode\n",
    "from black import out\n",
    "from fastNLP import EarlyStopCallback\n",
    "from pyrsistent import b\n",
    "\n",
    "import numpy as np\n",
    "import timm\n",
    "import timm.data\n",
    "import timm.loss\n",
    "import timm.optim\n",
    "import timm.utils\n",
    "import torch\n",
    "import torchmetrics\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "\n",
    "from pytorch_accelerated.trainer import Trainer, DEFAULT_CALLBACKS\n",
    "from timm.data.parsers.parser import Parser\n",
    "from pathlib import Path\n",
    "\n",
    "from torchmetrics import MetricCollection, Accuracy, Precision, Recall\n",
    "\n",
    "from pytorch_accelerated.callbacks import TrainerCallback, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentLog():\n",
    "    def __init__(self):\n",
    "        self.path = \"experiment-reports/\"\n",
    "        self.ext = \".log\"\n",
    "        self.model_name = \"\"\n",
    "        self.drop_path_rate = \"\"\n",
    "        self.epoch = \"\"\n",
    "        self.image_size = \"\"\n",
    "        self.learning_rate = \"\"\n",
    "        self.bce_target_thresh = \"\"\n",
    "        self.smoothing = \"\"\n",
    "        self.mixup = \"\"\n",
    "        self.cutmix = \"\"\n",
    "        self.batch_size = \"\"\n",
    "        self.loss_name = \"\"\n",
    "        self.auto_augment = \"\"\n",
    "        self.data_mean = \"\"\n",
    "        self.data_std = \"\"\n",
    "        self.optimizer = \"\"\n",
    "        self.weight_decay = \"\"\n",
    "        self.accuracy = \"\"\n",
    "        self.precision = \"\"\n",
    "        self.recall = \"\"\n",
    "        self.early_stop = \"\"\n",
    "\n",
    "    def write_experiment_details(self, filename):\n",
    "        filename = filename\n",
    "        f = open(self.path + filename + self.ext, \"w\")\n",
    "        f.write(\"Model Name: \"+self.model_name)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Drop Rate: \"+self.drop_path_rate)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Epochs Trained: \"+self.epoch)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Image Size: \"+self.image_size)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Learning Rate: \"+self.learning_rate)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"BCE Thresh: \"+self.bce_target_thresh)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Smoothing: \"+self.smoothing)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Mixup: \"+self.mixup)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Cutmix: \"+self.cutmix)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Batch Size: \"+self.batch_size)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Loss: \"+self.loss_name)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Auto Augment: \"+self.auto_augment)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Data Mean: \"+self.data_mean)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Data Std: \"+self.data_std)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Optimizer: \"+self.optimizer)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Weight Decay: \"+self.weight_decay)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Early Stop: \"+self.early_stop)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Best Accuracy: \"+self.accuracy)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Best Acc-Precision: \"+self.precision)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Best Acc-Recall: \"+self.recall)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(image_size, data_mean, data_std, train_path, val_path, log):\n",
    "    auto_augment = \"rand-m7-mstd0.5-inc1\"\n",
    "    log.auto_augment = auto_augment\n",
    "\n",
    "    train_transforms = timm.data.create_transform(\n",
    "        input_size=image_size,\n",
    "        is_training=True,\n",
    "        mean=data_mean,\n",
    "        std=data_std,\n",
    "        auto_augment= auto_augment,\n",
    "    )\n",
    "\n",
    "    eval_transforms = timm.data.create_transform(\n",
    "        input_size=image_size, mean=data_mean, std=data_std\n",
    "    )\n",
    "\n",
    "    train_dataset = timm.data.dataset.ImageDataset(\n",
    "        train_path, transform=train_transforms\n",
    "    )\n",
    "    eval_dataset = timm.data.dataset.ImageDataset(val_path, transform=eval_transforms)\n",
    "\n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "best_since = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveBestModelCallback(TrainerCallback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        log,\n",
    "        watch_metric=\"eval_loss_epoch\",\n",
    "        greater_is_better: bool = False,\n",
    "        reset_on_train: bool = True,\n",
    "    ):\n",
    "        self.watch_metric = watch_metric\n",
    "        self.greater_is_better = greater_is_better\n",
    "        self.operator = np.greater if self.greater_is_better else np.less\n",
    "        self.best_metric = None\n",
    "        self.save_path = log.model_name+\".pt\"\n",
    "        self.reset_on_train = reset_on_train\n",
    "        self.log = log\n",
    "\n",
    "    def on_training_run_start(self, args, **kwargs):\n",
    "        if self.reset_on_train:\n",
    "            self.best_metric = None\n",
    "\n",
    "    def on_training_run_epoch_end(self, trainer, **kwargs):\n",
    "        current_metric = trainer.run_history.get_latest_metric(self.watch_metric)\n",
    "\n",
    "        if self.best_metric is None:\n",
    "            self.best_metric = current_metric\n",
    "            trainer.save_checkpoint(\n",
    "                save_path=self.save_path,\n",
    "                checkpoint_kwargs={self.watch_metric: self.best_metric},\n",
    "            )\n",
    "\n",
    "            self.log.accuracy = str(trainer.run_history.get_latest_metric(\"accuracy\"))\n",
    "            self.log.precision = str(trainer.run_history.get_latest_metric(\"precision\"))\n",
    "            self.log.recall = str(trainer.run_history.get_latest_metric(\"recall\"))\n",
    "            \n",
    "        else:\n",
    "            is_improvement = self.operator(current_metric, self.best_metric)\n",
    "\n",
    "            if is_improvement:\n",
    "                self.best_metric = current_metric\n",
    "                trainer.save_checkpoint(\n",
    "                    save_path=self.save_path,\n",
    "                    checkpoint_kwargs={\"loss\": self.best_metric},\n",
    "                )\n",
    "\n",
    "                self.log.accuracy = str(trainer.run_history.get_latest_metric(\"accuracy\"))\n",
    "                self.log.precision = str(trainer.run_history.get_latest_metric(\"precision\"))\n",
    "                self.log.recall = str(trainer.run_history.get_latest_metric(\"recall\"))\n",
    "\n",
    "\n",
    "    def on_training_run_end(self, trainer, **kwargs):\n",
    "        trainer.print(\n",
    "            f\"Loading checkpoint with {self.watch_metric}: {self.best_metric}\"\n",
    "        )\n",
    "        trainer.load_checkpoint(self.save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimmMixupTrainer(Trainer):\n",
    "    def __init__(self, eval_loss_fn, mixup_args, num_classes, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.eval_loss_fn = eval_loss_fn\n",
    "        self.num_updates = None\n",
    "        self.mixup_fn = timm.data.Mixup(**mixup_args)\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy(num_classes=num_classes, average=\"macro\")\n",
    "        self.precision = torchmetrics.Precision(num_classes=num_classes, average=\"macro\")\n",
    "        self.recall = torchmetrics.Recall(num_classes=num_classes, average=\"macro\")\n",
    "\n",
    "        self.ema_accuracy = torchmetrics.Accuracy(num_classes=num_classes)\n",
    "        self.ema_model = None\n",
    "\n",
    "    def create_scheduler(self):\n",
    "        return timm.scheduler.CosineLRScheduler(\n",
    "            self.optimizer,\n",
    "            t_initial=self.run_config.num_epochs,\n",
    "            cycle_decay=0.5,\n",
    "            lr_min=1e-6,\n",
    "            t_in_epochs=True,\n",
    "            warmup_t=3,\n",
    "            warmup_lr_init=1e-4,\n",
    "            cycle_limit=1,\n",
    "        )\n",
    "\n",
    "    def training_run_start(self):\n",
    "        # Model EMA requires the model without a DDP wrapper and before sync batchnorm conversion\n",
    "        self.ema_model = timm.utils.ModelEmaV2(\n",
    "            self._accelerator.unwrap_model(self.model), decay=0.9\n",
    "        )\n",
    "        if self.run_config.is_distributed:\n",
    "            self.model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(self.model)\n",
    "\n",
    "    def train_epoch_start(self):\n",
    "        super().train_epoch_start()\n",
    "        self.num_updates = self.run_history.current_epoch * len(self._train_dataloader)\n",
    "\n",
    "    def calculate_train_batch_loss(self, batch):\n",
    "        xb, yb = batch\n",
    "        mixup_xb, mixup_yb = self.mixup_fn(xb, yb)\n",
    "        return super().calculate_train_batch_loss((mixup_xb, mixup_yb))\n",
    "\n",
    "    def train_epoch_end(\n",
    "        self,\n",
    "    ):\n",
    "        self.ema_model.update(self.model)\n",
    "        self.ema_model.eval()\n",
    "\n",
    "        if hasattr(self.optimizer, \"sync_lookahead\"):\n",
    "            self.optimizer.sync_lookahead()\n",
    "\n",
    "    def scheduler_step(self):\n",
    "        self.num_updates += 1\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step_update(num_updates=self.num_updates)\n",
    "\n",
    "    def calculate_eval_batch_loss(self, batch):\n",
    "        with torch.no_grad():\n",
    "            xb, yb = batch\n",
    "            outputs = self.model(xb)\n",
    "            val_loss = self.eval_loss_fn(outputs, yb)\n",
    "            self.accuracy.update(outputs.argmax(-1), yb)\n",
    "            self.precision.update(outputs.argmax(-1), yb)\n",
    "            self.recall.update(outputs.argmax(-1), yb)\n",
    "\n",
    "            ema_model_preds = self.ema_model.module(xb).argmax(-1)\n",
    "            self.ema_accuracy.update(ema_model_preds, yb)\n",
    "\n",
    "        return {\"loss\": val_loss, \"model_outputs\": outputs, \"batch_size\": xb.size(0)}\n",
    "\n",
    "    def eval_epoch_end(self):\n",
    "        super().eval_epoch_end()\n",
    "\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step(self.run_history.current_epoch + 1)\n",
    "\n",
    "        self.run_history.update_metric(\"accuracy\", self.accuracy.compute().cpu())\n",
    "        self.run_history.update_metric(\"precision\", self.precision.compute().cpu())\n",
    "        self.run_history.update_metric(\"recall\", self.recall.compute().cpu())\n",
    "\n",
    "        self.run_history.update_metric(\n",
    "            \"ema_model_accuracy\", self.ema_accuracy.compute().cpu()\n",
    "        )\n",
    "        self.accuracy.reset()\n",
    "        self.precision.reset()\n",
    "        self.recall.reset()\n",
    "        self.ema_accuracy.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = ExperimentLog()\n",
    "# Set training arguments, hardcoded here for clarity\n",
    "image_size = (224, 224)\n",
    "lr = 5e-3\n",
    "smoothing = 0.1\n",
    "mixup = 0.2\n",
    "cutmix = 1.0\n",
    "batch_size = 128\n",
    "bce_target_thresh = 0.2\n",
    "num_epochs = 50\n",
    "early_stop = 5\n",
    "\n",
    "log.image_size = str(image_size)\n",
    "log.learning_rate = str(lr)\n",
    "log.smoothing = str(smoothing)\n",
    "log.mixup = str(mixup)\n",
    "log.cutmix = str(cutmix)\n",
    "log.batch_size = str(batch_size)\n",
    "log.bce_target_thresh = str(bce_target_thresh)\n",
    "log.epoch = str(num_epochs)\n",
    "log.early_stop = str(early_stop)\n",
    "\n",
    "data_path = Path(\"../dataset/sample/\")\n",
    "train_path = data_path / \"train\"\n",
    "val_path = data_path / \"valid\"\n",
    "num_classes = len(list(train_path.iterdir()))\n",
    "\n",
    "mixup_args = dict(\n",
    "    mixup_alpha=mixup,\n",
    "    cutmix_alpha=cutmix,\n",
    "    label_smoothing=smoothing,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "\n",
    "model_name = \"efficientnet_b0\"\n",
    "drop_path_rate = 0.05\n",
    "log.model_name = model_name\n",
    "log.drop_path_rate = str(drop_path_rate)\n",
    "# Create model using timm\n",
    "model = timm.create_model(\n",
    "    model_name=model_name, pretrained=True, num_classes=num_classes, drop_path_rate=drop_path_rate\n",
    ")\n",
    "\n",
    "# Load data config associated with the model to use in data augmentation pipeline\n",
    "data_config = timm.data.resolve_data_config({}, model=model, verbose=True)\n",
    "# mean: tensor([0.4124, 0.3856, 0.3493])\n",
    "# std:  tensor([0.2798, 0.2703, 0.2726])\n",
    "data_mean = (0.4124, 0.3856, 0.3493)\n",
    "data_std = (0.2798, 0.2703, 0.2726)\n",
    "log.data_mean = str(data_mean)\n",
    "log.data_std = str(data_std)\n",
    "# Create training and validation datasets\n",
    "train_dataset, eval_dataset = create_datasets(\n",
    "    train_path=train_path,\n",
    "    val_path=val_path,\n",
    "    image_size=image_size,\n",
    "    data_mean=data_mean,\n",
    "    data_std=data_std,\n",
    "    log= log\n",
    ")\n",
    "\n",
    "optimizer_name = \"lookahead_AdamW\"\n",
    "weight_decay = 0.01\n",
    "log.optimizer = optimizer_name\n",
    "log.weight_decay = str(weight_decay)\n",
    "# Create optimizer\n",
    "optimizer = timm.optim.create_optimizer_v2(\n",
    "    model, opt= optimizer_name, lr=lr, weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "# As we are using Mixup, we can use BCE during training and CE for evaluation\n",
    "train_loss_fn = torch.nn.CrossEntropyLoss(\n",
    ")\n",
    "validate_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "log.loss_name = str(train_loss_fn)\n",
    "\n",
    "# Create trainer and start training\n",
    "trainer = TimmMixupTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=train_loss_fn,\n",
    "    eval_loss_fn=validate_loss_fn,\n",
    "    mixup_args=mixup_args,\n",
    "    num_classes=num_classes,\n",
    "    callbacks=[\n",
    "        *DEFAULT_CALLBACKS,\n",
    "        SaveBestModelCallback(\n",
    "            watch_metric=\"accuracy\", \n",
    "            greater_is_better=True, \n",
    "            log =log),\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=early_stop, \n",
    "            early_stopping_threshold=0.001,\n",
    "            watch_metric=\"accuracy\",\n",
    "            greater_is_better=True)\n",
    "        ],\n",
    ")\n",
    "\n",
    "trainer.train(\n",
    "    per_device_batch_size=batch_size,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    num_epochs=num_epochs,\n",
    "    create_scheduler_fn=trainer.create_scheduler,\n",
    ")\n",
    "\n",
    "log.write_experiment_details(log.model_name+\"-\"+str(log.accuracy))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3e56b6e42d9806706b48a0cb377354a7829f942606da048651ec030dcfc2b5d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('timm-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
