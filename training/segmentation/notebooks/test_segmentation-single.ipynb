{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "facd5880-3a77-41c8-b54c-3beecc609538",
   "metadata": {},
   "source": [
    "# Test segmentation model on a single image and get ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c28a19-01c9-49b3-8a29-a5ad09148b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d84cd-bb50-4d06-8b91-5e2b18452f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc6923a-1890-4c06-bf88-13539e0e7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from skimage.util import img_as_ubyte\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters.rank import entropy\n",
    "from skimage.morphology import disk\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "\n",
    "data_folder = 'datasets/Auto-retail-syndata-release'\n",
    "model_path = 'logs/unet_aicityt4/unet_aicityt4.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20de3cc9-3286-48c3-9455-0cb1e3520b50",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144649f3-73e1-4b4a-a515-919fb1b56528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "###### UNet model\n",
    "#########################################################################################\n",
    "\"\"\" Convolutional block:\n",
    "    It follows a two 3x3 convolutional layer, each followed by a batch normalization and a relu activation.\n",
    "\"\"\"\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\"\"\" Encoder block:\n",
    "    It consists of an conv_block followed by a max pooling.\n",
    "    Here the number of filters doubles and the height and width half after every block.\n",
    "\"\"\"\n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = conv_block(in_c, out_c)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "\n",
    "        return x, p\n",
    "\n",
    "\"\"\" Decoder block:\n",
    "    The decoder block begins with a transpose convolution, followed by a concatenation with the skip\n",
    "    connection from the encoder block. Next comes the conv_block.\n",
    "    Here the number filters decreases by half and the height and width doubles.\n",
    "\"\"\"\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(out_c+out_c, out_c)\n",
    "\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class build_unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.e1 = encoder_block(3, 64)\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)\n",
    "\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.b = conv_block(512, 1024)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64)\n",
    "\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
    "        \n",
    "        # NOTE: \n",
    "        # nn.Conv2d(64, 1, kernel_size=1, padding=0) is mathematically same as \n",
    "        # nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        s1, p1 = self.e1(inputs)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(d4)\n",
    "        return outputs\n",
    "######################################################################################### \n",
    "\n",
    "# Define model\n",
    "segmentation_model = build_unet()\n",
    "checkpoint = torch.load(model_path)\n",
    "segmentation_model.load_state_dict(checkpoint)\n",
    "# Send to GPU\n",
    "segmentation_model = segmentation_model.to(DEVICE)\n",
    "segmentation_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c84dc0-6182-4b5d-9037-9a016900917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(idx, **images):\n",
    "    \"\"\"Plot images in one row.\"\"\" \n",
    "    n = len(images)\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).lower(), fontsize=20)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis(\"off\")\n",
    "        #plt.tight_layout()\n",
    "    #plt.savefig(\"../outputs/vis/compare-segs/{}.png\".format(idx), facecolor=\"white\", bbox_inches = 'tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20827884-c73d-43c7-93f1-915d5e99f361",
   "metadata": {},
   "source": [
    "## Make prediction on single image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7aa6fd-fbfa-4e21-816a-9b37fa2d6c12",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1. Hand segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2bf135-f3f3-453b-92a3-f8607033813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To use this block, run `pip install pytorch-lightning`\n",
    "#Reference: https://github.com/guglielmocamporese/hands-segmentation-pytorch\n",
    "\n",
    "# Imports\n",
    "import torch\n",
    "import torch.hub\n",
    "\n",
    "# Create the model\n",
    "hand_seg_model = torch.hub.load(\n",
    "    repo_or_dir='guglielmocamporese/hands-segmentation-pytorch', \n",
    "    model='hand_segmentor', \n",
    "    pretrained=True\n",
    ")\n",
    "hand_seg_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c4dcb-1b89-4cbb-9d29-b27cac982169",
   "metadata": {},
   "source": [
    "#### 2. Product segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fbbee2-1f0b-4155-be03-3fd7dbf39b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_with_hand_seg(img):\n",
    "    \"\"\"\n",
    "    Crops product image using product and hand segmentation.\n",
    "    \"\"\"\n",
    "        \n",
    "    image_b = torch.unsqueeze(img, 0)\n",
    "    image = (image_b[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    pred = hand_seg_model(image_b).argmax(1).detach().cpu().numpy().squeeze()\n",
    "    # invert mask, use plt.imshow(hand_pred) for vis\n",
    "    hand_pred = np.logical_not(pred).astype(int) \n",
    "    \n",
    "    # Stack preds for getting ROI\n",
    "    hand_pred3d = np.stack((hand_pred, hand_pred, hand_pred), axis=-1)\n",
    "    # Get rid off hands\n",
    "    image_no_hands = np.array(image) * hand_pred3d # use plt.imshow(image_no_hands) for vis\n",
    "    \n",
    "    # Convert new image to tensor\n",
    "    image_no_hands = Image.fromarray(np.uint8(image_no_hands)).convert('RGB')\n",
    "    image_tensor = transforms_image(image_no_hands)\n",
    "\n",
    "    image_tensor_b = torch.unsqueeze(image_tensor, 0).to(DEVICE)\n",
    "    output = torch.sigmoid(segmentation_model(image_tensor_b.float()))\n",
    "    pred = output.permute(0, 2, 3, 1).squeeze().detach().cpu().numpy() > 0.5\n",
    "    \n",
    "    # Stack preds for getting ROI\n",
    "    mask = np.stack((pred, pred, pred), axis=-1)\n",
    "    \n",
    "    # Image ROI\n",
    "    image_roi = image_no_hands * mask\n",
    "    \n",
    "    data = {\"roi\": image_roi,\n",
    "            \"mask\": pred,\n",
    "            \"image\": image\n",
    "           }\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def crop_without_hand_seg(img):\n",
    "    \"\"\"\n",
    "    Crops product image using product segmentation.\n",
    "    \"\"\"\n",
    "    image_tensor_b = torch.unsqueeze(img, 0).to(DEVICE)\n",
    "    output = torch.sigmoid(segmentation_model(image_tensor_b.float()))\n",
    "    pred = output.permute(0, 2, 3, 1).squeeze().detach().cpu().numpy() > 0.5\n",
    "    \n",
    "    # Stack preds for getting ROI\n",
    "    mask = np.stack((pred, pred, pred), axis=-1)\n",
    "    \n",
    "    # Image ROI\n",
    "    image = (image_tensor_b[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    image_roi = image * mask\n",
    "    \n",
    "    data = {\"roi\": image_roi,\n",
    "            \"mask\": pred,\n",
    "            \"image\": image\n",
    "           }\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def entropy_based_seg(img):\n",
    "    image_gray = rgb2gray(img)\n",
    "    image_gray = img_as_ubyte(image_gray)\n",
    "    entropy_image = entropy(image_gray, disk(6)) # 6 default\n",
    "    scaled_entropy = entropy_image / entropy_image.max()\n",
    "    threshold = scaled_entropy > 0.8 # 0.8 default\n",
    "    image_seg = np.dstack([img[:,:,0]*threshold,\n",
    "                            img[:,:,1]*threshold,\n",
    "                            img[:,:,2]*threshold])\n",
    "    return image_seg\n",
    "\n",
    "\n",
    "def crop_with_hand_entropy_seg(img):\n",
    "    \"\"\"\n",
    "    Crops product image using product, hand and entropy based segmentation.\n",
    "    \n",
    "    Reference: https://towardsdatascience.com/image-processing-with-python-working-with-entropy-b05e9c84fc36\n",
    "    \"\"\"\n",
    "        \n",
    "    image_b = torch.unsqueeze(img, 0)\n",
    "    image = (image_b[0].permute(1,2,0).detach().cpu().numpy()+1)/2\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    \n",
    "    # Hand segmentation\n",
    "    pred = hand_seg_model(image_b).argmax(1).detach().cpu().numpy().squeeze()\n",
    "    # invert mask, use plt.imshow(hand_pred) for vis\n",
    "    hand_pred = np.logical_not(pred).astype(int) \n",
    "    \n",
    "    # Stack preds for getting ROI\n",
    "    hand_pred3d = np.stack((hand_pred, hand_pred, hand_pred), axis=-1)\n",
    "    # Get rid off hands\n",
    "    image_no_hands = np.array(image) * hand_pred3d # use plt.imshow(image_no_hands) for vis\n",
    "    \n",
    "    # Convert new image to tensor\n",
    "    image_no_hands = Image.fromarray(np.uint8(image_no_hands)).convert('RGB')\n",
    "    image_tensor = transforms_image(image_no_hands)\n",
    "\n",
    "    image_tensor_b = torch.unsqueeze(image_tensor, 0).to(DEVICE)\n",
    "    output = torch.sigmoid(segmentation_model(image_tensor_b.float()))\n",
    "    pred = output.permute(0, 2, 3, 1).squeeze().detach().cpu().numpy() > 0.5\n",
    "    \n",
    "    # Stack preds for getting ROI\n",
    "    mask = np.stack((pred, pred, pred), axis=-1)\n",
    "    \n",
    "    # Image ROI\n",
    "    image_roi = image_no_hands * mask\n",
    "    \n",
    "    # Entropy based segmentation\n",
    "    image_roi = entropy_based_seg(image_roi)\n",
    "    \n",
    "    data = {\"roi\": image_roi,\n",
    "            \"mask\": pred,\n",
    "            \"image\": image\n",
    "           }\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "transforms_image = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                       transforms.CenterCrop((224, 224)),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab5a15d-fc00-4cc1-ba32-4b6f72b0ebac",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84afac60-125b-4e53-8ef3-1cdafdd0a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    \n",
    "    image = Image.open(path).convert('RGB')\n",
    "    \n",
    "    # Taken from AICITYSeg_dataloader dataloader class\n",
    "    transforms_image = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                           transforms.CenterCrop((224, 224)),\n",
    "                                           transforms.ToTensor(),\n",
    "                                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    image_tensor = transforms_image(image)\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cddd13f-46e2-401b-ba10-7e9880031127",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = [ \"datasets/test_images/image21.jpeg\",\n",
    "             \"datasets/test_images/image23.jpeg\",\n",
    "             \"datasets/test_images/image24.jpeg\",\n",
    "             \"datasets/test_images/image11.jpeg\",\n",
    "             \"datasets/test_images/image22.jpeg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a10af-1fbf-4420-8916-24faec903182",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in img_paths:\n",
    "    data1 = crop_with_hand_seg(load_image(path))\n",
    "    data2 = crop_without_hand_seg(load_image(path))\n",
    "    \n",
    "    # Use this, data[\"roi\"]\n",
    "    data3 = crop_with_hand_entropy_seg(load_image(path))\n",
    "\n",
    "    visualize(0, input_image=data1[\"image\"], segmented_without_hand=data2[\"roi\"], segmented_with_hand=data1[\"roi\"], segmented_with_hand_entropy=data3['roi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de6dc3c-b1d0-491b-978b-902d4a3a2add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40596976-2221-4115-a3f2-ecb2884f9e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e18bfb7-a7f5-48f7-a276-a9a6bde74d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
