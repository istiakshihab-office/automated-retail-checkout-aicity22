{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# %load \"../segmentation.py\"\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Imports segmenter for getting ROI from frames\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from segmenter import crop_with_hand_entropy_seg\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"vit_base_patch32_224\", num_classes=116)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = torch.load(\"../models/vit_base_patch32_224.pt\", map_location=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(trained_model[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../labels.txt\") as f:\n",
    "    prod = []\n",
    "    for line in f.readlines():\n",
    "        prod.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(image_path,model):\n",
    "   img = Image.open(image_path)\n",
    "   mean = [0.485, 0.456, 0.406] \n",
    "   std = [0.229, 0.224, 0.225]\n",
    "   test_transforms = timm.data.create_transform(\n",
    "      input_size=224, mean=mean, std=std\n",
    "   )\n",
    "   img_normalized = test_transforms(img).float()\n",
    "   img_normalized = img_normalized.unsqueeze_(0)\n",
    "   img_normalized = img_normalized.to(\"cpu\")\n",
    "   with torch.no_grad():\n",
    "      model.eval()  \n",
    "      output =model(img_normalized)\n",
    "      index = output.data.cpu().numpy().argmax()\n",
    "      return prod[index].strip(), output\n",
    "\n",
    "def infer_video(frame,model):\n",
    "   frame = frame[256:812,512:1216,]\n",
    "   img = Image.fromarray(np.uint8(frame)).convert('RGB')\n",
    "   mean = [0.485, 0.456, 0.406] \n",
    "   std = [0.229, 0.224, 0.225]\n",
    "   test_transforms = timm.data.create_transform(\n",
    "      input_size=224, mean=mean, std=std\n",
    "   )\n",
    "   img_normalized = test_transforms(img).float()\n",
    "   img_normalized = img_normalized.unsqueeze_(0)\n",
    "   img_normalized = img_normalized.to(\"cpu\")\n",
    "   with torch.no_grad():\n",
    "      model.eval()  \n",
    "      output =model(img_normalized)\n",
    "      return output.max(), frame, frame.mean()\n",
    "\n",
    "def get_ratio(image):\n",
    "    h, w, _ = image.shape\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU + cv2.THRESH_BINARY_INV)[1]\n",
    "\n",
    "    pixels = cv2.countNonZero(thresh)\n",
    "    ratio = (pixels/(h * w)) * 100\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_colorfulness(image):\n",
    "\t# split the image into its respective RGB components\n",
    "\t(B, G, R) = cv2.split(image.astype(\"float\"))\n",
    "\t# compute rg = R - G\n",
    "\trg = np.absolute(R - G)\n",
    "\t# compute yb = 0.5 * (R + G) - B\n",
    "\tyb = np.absolute(0.5 * (R + G) - B)\n",
    "\t# compute the mean and standard deviation of both `rg` and `yb`\n",
    "\t(rbMean, rbStd) = (np.mean(rg), np.std(rg))\n",
    "\t(ybMean, ybStd) = (np.mean(yb), np.std(yb))\n",
    "\t# combine the mean and standard deviations\n",
    "\tstdRoot = np.sqrt((rbStd ** 2) + (ybStd ** 2))\n",
    "\tmeanRoot = np.sqrt((rbMean ** 2) + (ybMean ** 2))\n",
    "\t# derive the \"colorfulness\" metric and return it\n",
    "\treturn stdRoot + (0.3 * meanRoot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic brightness and contrast optimization with optional histogram clipping\n",
    "def automatic_brightness_and_contrast(image, clip_hist_percent=1):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate grayscale histogram\n",
    "    hist = cv2.calcHist([gray],[0],None,[256],[0,256])\n",
    "    hist_size = len(hist)\n",
    "    \n",
    "    # Calculate cumulative distribution from the histogram\n",
    "    accumulator = []\n",
    "    accumulator.append(float(hist[0]))\n",
    "    for index in range(1, hist_size):\n",
    "        accumulator.append(accumulator[index -1] + float(hist[index]))\n",
    "    \n",
    "    # Locate points to clip\n",
    "    maximum = accumulator[-1]\n",
    "    clip_hist_percent *= (maximum/100.0)\n",
    "    clip_hist_percent /= 2.0\n",
    "    \n",
    "    # Locate left cut\n",
    "    minimum_gray = 0\n",
    "    while accumulator[minimum_gray] < clip_hist_percent:\n",
    "        minimum_gray += 1\n",
    "    \n",
    "    # Locate right cut\n",
    "    maximum_gray = hist_size -1\n",
    "    while accumulator[maximum_gray] >= (maximum - clip_hist_percent):\n",
    "        maximum_gray -= 1\n",
    "    \n",
    "    # Calculate alpha and beta values\n",
    "    alpha = 255 / (maximum_gray - minimum_gray)\n",
    "    beta = -minimum_gray * alpha\n",
    "    \n",
    "    '''\n",
    "    # Calculate new histogram with desired range and show histogram \n",
    "    new_hist = cv2.calcHist([gray],[0],None,[256],[minimum_gray,maximum_gray])\n",
    "    plt.plot(hist)\n",
    "    plt.plot(new_hist)\n",
    "    plt.xlim([0,256])\n",
    "    plt.show()\n",
    "    '''\n",
    "\n",
    "    auto_result = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "    return auto_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "video_location = \"../test-videos/\"\n",
    "videos = [\"testA_1.mp4\"]\n",
    "images_list = []\n",
    "ratios_list = []\n",
    "colors_list = []\n",
    "metrics_list = []\n",
    "\n",
    "for video in videos:\n",
    "  vidcap = cv2.VideoCapture(video_location+video)\n",
    "  fps = vidcap.get(cv2.CAP_PROP_FPS)      \n",
    "  frame_count = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "  resized_image = []\n",
    "\n",
    "  for i in tqdm(range(frame_count)):\n",
    "    try:\n",
    "      success,image = vidcap.read()\n",
    "      image = image[256:896, 512:1400]\n",
    "      image = automatic_brightness_and_contrast(image)\n",
    "      image = cv2.resize(image, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "      resized_image.append(image)\n",
    "    except:\n",
    "      pass\n",
    "  ratios = []\n",
    "  colors = []\n",
    "  metrics = []\n",
    "  for img in resized_image:\n",
    "    # ratio = get_ratio(img)\n",
    "    color = image_colorfulness(img)\n",
    "    # metric = color*color*ratio\n",
    "    colors.append(color)\n",
    "    # ratios.append(ratio)\n",
    "    # metrics.append(metric)\n",
    "  \n",
    "  images_list.append(resized_image)\n",
    "  # ratios_list.append(ratios)\n",
    "  colors_list.append(colors)\n",
    "  # metrics_list.append(metrics)\n",
    "  # print(video)\n",
    "  # plt.plot(metrics)\n",
    "  # plt.show()\n",
    "  # plt.plot(ratios)\n",
    "  # plt.show()\n",
    "  plt.plot(colors)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.fftpack as fftpack\n",
    "\n",
    "def get_sharpness(image):\n",
    "    im = Image.fromarray(image).convert('L') # to grayscale\n",
    "    array = np.asarray(im, dtype=np.int32)\n",
    "\n",
    "    gy, gx = np.gradient(array)\n",
    "    gnorm = np.sqrt(gx**2 + gy**2)\n",
    "    sharpness = np.average(gnorm)\n",
    "    return sharpness\n",
    "\n",
    "def smooth_data_fft(arr, span):  # the scaling of \"span\" is open to suggestions\n",
    "    w = fftpack.rfft(arr)\n",
    "    spectrum = w ** 2\n",
    "    cutoff_idx = spectrum < (spectrum.max() * (1 - np.exp(-span / 2000)))\n",
    "    w[cutoff_idx] = 0\n",
    "    return fftpack.irfft(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter, argrelextrema,find_peaks\n",
    "import math\n",
    "images_with_object = []\n",
    "idx = []\n",
    "count = 1\n",
    "for images, colors in zip(images_list, colors_list):\n",
    "    frames_with_object = []\n",
    "    rolling_colors = smooth_data_fft(colors, 20)\n",
    "    maximas = find_peaks(rolling_colors)\n",
    "    for maxima in maximas[0]:\n",
    "        candidate_frame = maxima\n",
    "        max_sharpness = -1\n",
    "        max_metric = -1\n",
    "        for i in range(-21,22,7):\n",
    "            current_frame = maxima+i\n",
    "            if(current_frame>=len(images)):\n",
    "                continue\n",
    "            sharpness = get_sharpness(images[current_frame])\n",
    "            colorfulness = image_colorfulness(images[current_frame])\n",
    "            ratio = get_ratio(images[current_frame])\n",
    "            metric = colorfulness*colorfulness * ratio\n",
    "            metric = math.sqrt(metric)\n",
    "            if(sharpness>max_sharpness and metric>max_metric):\n",
    "                max_sharpness = sharpness\n",
    "                max_metric = metric\n",
    "                candidate_frame = current_frame\n",
    "        if(max_metric>110):\n",
    "            frames_with_object.append((images[candidate_frame],candidate_frame))\n",
    "            idx.append(candidate_frame)\n",
    "            count+=1\n",
    "            plt.imshow(images[candidate_frame][:,:,[2,1,0]])\n",
    "            plt.show()\n",
    "\n",
    "            print(f\"Color {colorfulness}\")\n",
    "            print(f\"Sharp {sharpness}\")\n",
    "            print(f\"Metric {max_metric}\")\n",
    "\n",
    "    plt.plot(rolling_colors)\n",
    "    plt.show()\n",
    "    print(\"NEXT\")\n",
    "    images_with_object.append(frames_with_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "plt.subplot(131)\n",
    "plt.plot(colors)\n",
    "plt.xlabel(\"Frames Count\")\n",
    "plt.ylabel(\"Colorfulness Value\")\n",
    "plt.title(\"(a) Original Colorfulness Value Array\")\n",
    "plt.subplot(132)\n",
    "plt.plot(rolling_colors)\n",
    "plt.xlabel(\"Frames Count\")\n",
    "plt.ylabel(\"Colorfulness Value\")\n",
    "plt.title(\"(b) Filtered Colorfulness Value Array (FFT)\")\n",
    "plt.subplot(133)\n",
    "plt.plot(rolling_colors,\"-D\", markevery=idx)\n",
    "plt.xlabel(\"Frames Count\")\n",
    "plt.ylabel(\"Colorfulness Value\")\n",
    "plt.title(\"(c) Selected Frames (dots)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"colorfulness-fft.png\",dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blobs(image):\n",
    "    img = np.array(image)\n",
    "\n",
    "    # Convert you image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Threshold the image to extract only objects that are not black\n",
    "    # You need to use a one channel image, that's why the slice to get the first layer\n",
    "    tv, thresh = cv2.threshold(gray[:,:,0], 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Get the contours from your thresholded image\n",
    "    contours = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]\n",
    "\n",
    "    # Create a copy of the original image to display the output rectangles\n",
    "    output = img.copy()\n",
    "\n",
    "    # Loop through your contours calculating the bounding rectangles and plotting them\n",
    "    # for c in contours:\n",
    "    #     x, y, w, h = cv2.boundingRect(c)\n",
    "        # if(w*h>(224*244)/10):\n",
    "    c = max(contours, key=cv2.contourArea)\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    return x,y,w,h\n",
    "    # cv2.rectangle(output, (x,y), (x+w, y+h), (0, 0, 255), 2)\n",
    "    # # Display the output image\n",
    "    # plt.imshow(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transforms_image = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                          transforms.CenterCrop((224, 224)),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_image_classification(frame):\n",
    "   img = Image.fromarray(frame).convert('RGB')\n",
    "\n",
    "   mean = [0.4124, 0.3856, 0.3493] \n",
    "   std = [0.2798, 0.2703, 0.2726]\n",
    "   test_transforms = timm.data.create_transform(\n",
    "      input_size=224, mean=mean, std=std\n",
    "   )\n",
    "   img_normalized = test_transforms(img).float()\n",
    "   return img_normalized\n",
    "\n",
    "def infer_frame(frame, model):\n",
    "   img = Image.fromarray(frame).convert('RGB')\n",
    "   img = transforms_image(img)\n",
    "   segmented_image = crop_with_hand_entropy_seg(img)[\"roi\"]\n",
    "   x,y,w,h = get_blobs(segmented_image)\n",
    "   image_roi = frame[y:y+h, x:x+w]\n",
    "   plt.imshow(image_roi[:,:,[2,1,0]])\n",
    "   plt.show()\n",
    "   img_normalized = preprocess_image_classification(frame=image_roi[:,:,[2,1,0]])\n",
    "   img_normalized = img_normalized.unsqueeze_(0)\n",
    "   img_normalized = img_normalized.to(\"cpu\")\n",
    "   with torch.no_grad():\n",
    "      model.eval()\n",
    "      output =model(img_normalized)\n",
    "      index = output.data.cpu().numpy().argmax()\n",
    "      op_array = output.data.cpu().numpy()\n",
    "      print(op_array[0][index])\n",
    "      return index+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "video_id = []\n",
    "object_id = []\n",
    "timestamp = []\n",
    "for idx in range(len(images_with_object)):\n",
    "    lastoutput = \"\"\n",
    "    for image,frame in images_with_object[idx]:\n",
    "        output = infer_frame(image, model)\n",
    "        lastoutput=output\n",
    "        video_id.append(idx+1)\n",
    "        object_id.append(output)\n",
    "        print(prod[output-1])\n",
    "        timestamp.append(int(frame/60))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vit_duplication_removed.txt', 'w') as f:\n",
    "\n",
    "    prev_vid = -1\n",
    "    prev_obj = -1\n",
    "    for vid, obj, tim in zip(video_id, object_id, timestamp):\n",
    "        if(vid==prev_vid and obj==prev_obj):\n",
    "            continue\n",
    "        f.write(f'{vid} {obj} {tim}\\n')\n",
    "        prev_vid = vid\n",
    "        prev_obj = obj\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa884020ed65da342b2c717bde878a8b9bf503b8ca14673f10767b13c4c8aa94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
